Meeting notes:
AI Guild Meeting:

Objective: Get an ML server up and running

- Week 1/2 : Gauge interest
  1. Come up with ballpark budget estimate
  2. Enough people/$$$ to realistically cover it

Open to discussion

Proposed budget: $2k

Matt pledges ~$500

3 stages:
    0. Prepare an MVP with stuff at Noisebridge
    1. Share with Noisebridge at large -- feedback about design/usage
    - Dichotomy between single beefy GPU vs. multiple parallel (low-VRAM) GPU's
- Single case : Powerful network like stable diffusion
- Parallel case : multiple GPU's, not a single one
 2. Come up with specs
   - Talk to people with experience in building machines
- Contact peeps for 1080s
 3. Choose build + come up with cost
- Then fund it

Stage 0 : 1 week
Stage 1 : 1 week
Stage 2: 2 weeks
Stage 3: 4 weeks

Other options:
    - Using cloud cluster / instance?
    Could be done. Cloud services expensive enough that local install will pay for itself quickly, especially amortized.
    Lambda does have cheap GPU hosting though
       Also is an option if our initial 2070 system doesn't work
    - Performance requirements? 
      We don't know! And would like feedback. Let's get that first before making assumptions


AI's for next Wednesday (hehe):
    - Get list of companies to cold call for GPU donations (TJ, Matt for calling) 
    - Set up 2070 GPU instances (Stage 0) (TJ will be around Saturday)

Meeting adjourned!
OpenMV : https://openmv.io/
OpenMV
Small - Affordable - Expandable
The OpenMV project is about creating low-cost, extensible, Python powered, machine vision modules and aims at becoming the “Arduino of Machine Vision“.
Image
Image segmentation https://www.mathworks.com/discovery/image-segmentation.html
Image Segmentation
Image segmentation is a commonly used technique in digital image processing and analysis to partition an image into multiple parts or regions, often based on the characteristics of the pixels in the image.
Video segmentation https://analyticsindiamag.com/a-guide-to-video-object-segmentation-for-beginners/#:~:text=Video%20segmentation%2C%20or%20the%20partitioning,creation%2C%20to%20name%20a%20few.
Analytics India Magazine
Vijaysinh Lendave
A Guide to Video Object Segmentation for Beginners
Segmenting and tracking the objects of interest in the video is critical for effectively analyzing and using video big data. Computer vision requires two basic tasks - segmenting and tracking video objects.
A Guide to Video Object Segmentation for Beginners
MiVOS https://hkchengrex.github.io/MiVOS/
The computer vision, for reference: https://blog.superannotate.com/introduction-to-computer-vision/ 
SuperAnnotate Blog
Introduction to computer vision: History and applications
Heading into your computer vision journey? Look no further! Our subtle introduction to computer vision will set the start.
Introduction to computer vision: History and applications
Supervised vs. unsupervised learning + defnitions: https://www.ibm.com/cloud/blog/supervised-vs-unsupervised-learning
Supervised vs. Unsupervised Learning: What’s the Difference?
Image
MiVOS again, which is a hand annotation tool for image segmentation: https://hkchengrex.github.io/MiVOS/
Works best with bisection method:
- Start with first frame
- Do  middle frame
- Converges to a more accurate solution
False positives around dark objects near kitty (since kitty is black)
- Load in annotations from MiVOS (with false positive regions)
- Image/video clips here have one cat
- correct mask on first frame
- For each subsequent frame: Find parts of regions with highest overlap, discard the rest.
Extra video editing:
- see process-labels.ipynb
- Saves time in MiVOS
(Side note: segmentation will be with UNet, as it's easiest to set up)
Has two directories: image , for raw input, and labels, for the segmented images
Python:
- UNet class itself (easy to get)
- VideoDataset (to load into Pytorch)
- generate_weights (v important, because signal to noise ratio is low for rare classes and needs to be compensated)
(It works by weighting images with positive labels (or whichever is rarer) higher than less rare ones, usually by frequency)
Q: What if we just labeled images with cat in it?
- For production project, good question to ask
- In the blog, I explained that it will be in robotic mice trained to run away from cats
- Could just be frame with cats in it
- Benefit of bounding mask is how much image is cat => how close is cat
- Is it enough to know if cat is in frame
- More fine grained labels have advantage of also doing the generic form well
- Excellent smoke check 
IPyWidgets, IPyEvents provide HTML functionality in Python (HTML -> Python -> HTML lol)
Helps create interactive viewers
Preprocessing in main.ipynb to convert to grayscale && mask
Train / validation split
- Currently taking all frames in all videos, shuffling, and putting in train/test/validation
- Not kosher, because adjacent frames could be in training and test, resulting in leakage (basically trained on the test dataset, so it can overfit to those images)
- Solution would be to divide by clips
Q: How many images do you need?
- Good question; Just create a diverse validation dataset, and scale until the validation dataset is good
Q: Could this detect another cat?
- Probably not as it. But there's enough online cat data. Just downgrade image to imitate mobile cam, then use it as training data for multiple cats. 
Q: Is it easier to train on a personal cat or big dataset of cats?
- Rule of thumb is to start with big dataset (i.e. ImageNet) , fine tune on small dataset
- Better practice is to train on lots of cats, then focus on specific kitties
- Trained with only 1 batch size because of Out-of-memory issues
Q: Could you break a movie into individual characters, and divide by character individually?
- Called rotoscoping, could use segmentation / interaction tools
Q: How much work is it?
- Depends on model in data
Q: What about soap operas? We could do scenarios with characters swapped
- Character swapping is hard. 
- Tools are built into commercial video editing software. Not aware of great open-source tool. MiVOS is close for segmentation of "normal" (ie medium resolution) cameras
Q: Can it detect multiple things at once?
- Up to quite a few. Attempt image classification on a thousand objects
Q: What about larger images?
- Large image segmentation could be done through tiling (tiling is a hyperparameter though)
Unet architecture
- Has 2 labels (background, cat)
- input channels = 1 bc grayscale
- using instance normalization instead of batch because small batch
- leaky relus over regular relus , because it could work better, also personal experience at NIH for getting more percentage points
- Loss function is dense: sum of cross entropy loss over several images.
-- Cross entropy determines loss of one probability distribution to another (both distributions add up to 1) : predicted vs. correct "distribution" of 100% correct answer, 0% everything else 
-- Taking loss per-pixel, since each pixel can belong to one of two classes (background, cat) 
-- For each pixel: find error value, multiply by weight of class, sum weighted errors
Main loop
- GPU stuff 
- Run forward  + backward
- Occasionally run on validation to see how you're doing
- Visualize inputs and outputs for debugging (helps ex. if ground truth is messed up)
heyitsguay — 10/05/2022
https://arxiv.org/abs/1505.04597
arXiv.org
U-Net: Convolutional Networks for Biomedical Image Segmentation
There is large consent that successful training of deep networks requires
many thousand annotated training samples. In this paper, we present a network
and training strategy that relies on the...
Image
Summary : Conv net on the left to convert image to coarse, but high density features.
- For regular conv nets, you turn this to probabilities.
- UNet upsamples end of convnet + combine with input to show dense, per-pixel probabilities
-- Output is same level as input
-- Innovation is in skip features, where the same features used in downsampling are used for upsampling
What video should I record with camera?
- Works best to get public dataset, convert to visually similar to personal camera input. 
- Note: Neural nets are brittle. Neural nets will have performance drops on edge cases that you wouldn't intuit. Even a great network trained on clear images will fail on grainy images. This is because we ourselves are exposed to a lot, lot of images, grainy and good. 
- So keep as close to your expected domain as possible
Criteria:
- diverse images of cats in all places and orientations of the house
- Even images without cats , so it can determine when cat is not present
- Want to run it on the Arduino hardware
- Close to ground level (since on robotic mouse, it will be close to ground level)
- Done under several lighting conditions (things look different to nn's under lighting conditions)  -> https://www.gwern.net/Tanks
The Neural Net Tank Urban Legend
AI folklore tells a story about a neural network trained to detect tanks which instead learned to detect time of day; investigating, this probably never happened.
Image
-- This brittleness has improved over time. Switching to new camera + hardware only caused a minor hiccup in the performance, rather than breaking it
Next Time:
- Model compression , model optimization to get model working on tiny Arduino
